{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# silence the warning note\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "compression=dict(zlib=True,complevel=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.96 GiB for an array with shape (744, 38, 138, 135) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m chem \u001b[39m=\u001b[39m xr\u001b[39m.\u001b[39mopen_mfdataset(chemfiles,combine\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnested\u001b[39m\u001b[39m'\u001b[39m,concat_dim\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTSTEP\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m encoding\u001b[39m=\u001b[39m{var:compression \u001b[39mfor\u001b[39;00m var \u001b[39min\u001b[39;00m chem\u001b[39m.\u001b[39mdata_vars}\n\u001b[1;32m----> 5\u001b[0m chem\u001b[39m.\u001b[39;49mto_netcdf(\u001b[39m'\u001b[39;49m\u001b[39mD:/Download/May_chem.nc\u001b[39;49m\u001b[39m'\u001b[39;49m,encoding\u001b[39m=\u001b[39;49mencoding)\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\xarray\\core\\dataset.py:1903\u001b[0m, in \u001b[0;36mDataset.to_netcdf\u001b[1;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[0;32m   1900\u001b[0m     encoding \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1901\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackends\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m to_netcdf\n\u001b[1;32m-> 1903\u001b[0m \u001b[39mreturn\u001b[39;00m to_netcdf(  \u001b[39m# type: ignore  # mypy cannot resolve the overloads:(\u001b[39;49;00m\n\u001b[0;32m   1904\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1905\u001b[0m     path,\n\u001b[0;32m   1906\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   1907\u001b[0m     \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m,\n\u001b[0;32m   1908\u001b[0m     group\u001b[39m=\u001b[39;49mgroup,\n\u001b[0;32m   1909\u001b[0m     engine\u001b[39m=\u001b[39;49mengine,\n\u001b[0;32m   1910\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   1911\u001b[0m     unlimited_dims\u001b[39m=\u001b[39;49munlimited_dims,\n\u001b[0;32m   1912\u001b[0m     compute\u001b[39m=\u001b[39;49mcompute,\n\u001b[0;32m   1913\u001b[0m     multifile\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1914\u001b[0m     invalid_netcdf\u001b[39m=\u001b[39;49minvalid_netcdf,\n\u001b[0;32m   1915\u001b[0m )\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\xarray\\backends\\api.py:1239\u001b[0m, in \u001b[0;36mto_netcdf\u001b[1;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[39mif\u001b[39;00m multifile:\n\u001b[0;32m   1237\u001b[0m     \u001b[39mreturn\u001b[39;00m writer, store\n\u001b[1;32m-> 1239\u001b[0m writes \u001b[39m=\u001b[39m writer\u001b[39m.\u001b[39;49msync(compute\u001b[39m=\u001b[39;49mcompute)\n\u001b[0;32m   1241\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(target, BytesIO):\n\u001b[0;32m   1242\u001b[0m     store\u001b[39m.\u001b[39msync()\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\xarray\\backends\\common.py:171\u001b[0m, in \u001b[0;36mArrayWriter.sync\u001b[1;34m(self, compute)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marray\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mda\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39m# TODO: consider wrapping targets with dask.delayed, if this makes\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39m# for any discernible difference in perforance, e.g.,\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39m# targets = [dask.delayed(t) for t in self.targets]\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m delayed_store \u001b[39m=\u001b[39m da\u001b[39m.\u001b[39;49mstore(\n\u001b[0;32m    172\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msources,\n\u001b[0;32m    173\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtargets,\n\u001b[0;32m    174\u001b[0m     lock\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlock,\n\u001b[0;32m    175\u001b[0m     compute\u001b[39m=\u001b[39;49mcompute,\n\u001b[0;32m    176\u001b[0m     flush\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    177\u001b[0m     regions\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mregions,\n\u001b[0;32m    178\u001b[0m )\n\u001b[0;32m    179\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msources \u001b[39m=\u001b[39m []\n\u001b[0;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\dask\\array\\core.py:1235\u001b[0m, in \u001b[0;36mstore\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1233\u001b[0m \u001b[39melif\u001b[39;00m compute:\n\u001b[0;32m   1234\u001b[0m     store_dsk \u001b[39m=\u001b[39m HighLevelGraph(layers, dependencies)\n\u001b[1;32m-> 1235\u001b[0m     compute_as_if_collection(Array, store_dsk, map_keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1238\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\dask\\base.py:341\u001b[0m, in \u001b[0;36mcompute_as_if_collection\u001b[1;34m(cls, dsk, keys, scheduler, get, **kwargs)\u001b[0m\n\u001b[0;32m    339\u001b[0m schedule \u001b[39m=\u001b[39m get_scheduler(scheduler\u001b[39m=\u001b[39mscheduler, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m, get\u001b[39m=\u001b[39mget)\n\u001b[0;32m    340\u001b[0m dsk2 \u001b[39m=\u001b[39m optimization_function(\u001b[39mcls\u001b[39m)(dsk, keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 341\u001b[0m \u001b[39mreturn\u001b[39;00m schedule(dsk2, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\dask\\threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[1;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[0;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[1;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[0;32m     90\u001b[0m     pool\u001b[39m.\u001b[39;49msubmit,\n\u001b[0;32m     91\u001b[0m     pool\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[0;32m     92\u001b[0m     dsk,\n\u001b[0;32m     93\u001b[0m     keys,\n\u001b[0;32m     94\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[0;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39;49m_thread_get_id,\n\u001b[0;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39;49mpack_exception,\n\u001b[0;32m     97\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\dask\\local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[1;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    509\u001b[0m         _execute_task(task, data)  \u001b[39m# Re-execute locally\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         raise_exception(exc, tb)\n\u001b[0;32m    512\u001b[0m res, worker_id \u001b[39m=\u001b[39m loads(res_info)\n\u001b[0;32m    513\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m][key] \u001b[39m=\u001b[39m res\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\dask\\local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(exc, tb)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39mif\u001b[39;00m exc\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[0;32m    318\u001b[0m     \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 319\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\dask\\local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[1;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     task, data \u001b[39m=\u001b[39m loads(task_info)\n\u001b[1;32m--> 224\u001b[0m     result \u001b[39m=\u001b[39m _execute_task(task, data)\n\u001b[0;32m    225\u001b[0m     \u001b[39mid\u001b[39m \u001b[39m=\u001b[39m get_id()\n\u001b[0;32m    226\u001b[0m     result \u001b[39m=\u001b[39m dumps((result, \u001b[39mid\u001b[39m))\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\dask\\core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[1;34m(arg, cache, dsk)\u001b[0m\n\u001b[0;32m    115\u001b[0m     func, args \u001b[39m=\u001b[39m arg[\u001b[39m0\u001b[39m], arg[\u001b[39m1\u001b[39m:]\n\u001b[0;32m    116\u001b[0m     \u001b[39m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[39m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[39m# operations in-place.\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m(_execute_task(a, cache) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m args))\n\u001b[0;32m    120\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ishashable(arg):\n\u001b[0;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m arg\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\dask\\array\\chunk.py:418\u001b[0m, in \u001b[0;36mgetitem\u001b[1;34m(obj, index)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Getitem function\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \n\u001b[0;32m    399\u001b[0m \u001b[39mThis function creates a copy of the desired selection for array-like\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    415\u001b[0m \n\u001b[0;32m    416\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 418\u001b[0m     result \u001b[39m=\u001b[39m obj[index]\n\u001b[0;32m    419\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    420\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    421\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArray chunk size or shape is unknown. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    422\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPossible solution with x.compute_chunk_sizes()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.96 GiB for an array with shape (744, 38, 138, 135) and data type float32"
     ]
    }
   ],
   "source": [
    "chemfiles = ['F:/Data/Project_anqing/May/May_chem-1.nc',\n",
    "             'F:/Data/Project_anqing/May/May_chem-2.nc']\n",
    "chem = xr.open_mfdataset(chemfiles,combine='nested',concat_dim='TSTEP')\n",
    "encoding={var:compression for var in chem.data_vars}\n",
    "chem.to_netcdf('D:/Download/May_chem.nc',encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemfiles = ['F:/Data/Project_anqing/May/May_chem-1.nc',\n",
    "             'F:/Data/Project_anqing/May/May_chem-2.nc']\n",
    "metfiles = ['F:/Data/Project_anqing/May/May_met.nc',\n",
    "            'F:/Data/Project_anqing/May/May_presrh.nc']\n",
    "pafiles1 = ['F:/Data/Project_anqing/May/May_PA_O3-1.nc',\n",
    "           'F:/Data/Project_anqing/May/May_PA_O3-2.nc']\n",
    "pafiles2 = ['F:/Data/Project_anqing/May/May_PA_NOx-1.nc',\n",
    "           'F:/Data/Project_anqing/May/May_PA_NOx-2.nc']\n",
    "isamfiles1 = ['F:/Data/Project_anqing/May/May_ISAM_O3-1.nc',\n",
    "             'F:/Data/Project_anqing/May/May_ISAM_O3-2.nc']\n",
    "isamfiles2 = ['F:/Data/Project_anqing/May/May_ISAM_PM-1.nc',\n",
    "             'F:/Data/Project_anqing/May/May_ISAM_PM-2.nc']\n",
    "\n",
    "pa_O3 = xr.open_mfdataset(pafiles1,combine='nested',concat_dim='TSTEP')\n",
    "isam_O3 = xr.open_mfdataset(isamfiles1,combine='nested',concat_dim='TSTEP')\n",
    "pa_NOx = xr.open_mfdataset(pafiles2,combine='nested',concat_dim='TSTEP')\n",
    "isam_PM = xr.open_mfdataset(isamfiles2,combine='nested',concat_dim='TSTEP')\n",
    "met = xr.open_mfdataset(metfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.96 GiB for an array with shape (744, 38, 138, 135) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m compression\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(zlib\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,complevel\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m      3\u001b[0m encoding\u001b[39m=\u001b[39m{var:compression \u001b[39mfor\u001b[39;00m var \u001b[39min\u001b[39;00m chem\u001b[39m.\u001b[39mdata_vars}\n\u001b[1;32m----> 4\u001b[0m chem\u001b[39m.\u001b[39;49mto_netcdf(\u001b[39m'\u001b[39;49m\u001b[39mD:/Download/May_chem.nc\u001b[39;49m\u001b[39m'\u001b[39;49m,encoding\u001b[39m=\u001b[39;49mencoding)\n\u001b[0;32m      6\u001b[0m encoding\u001b[39m=\u001b[39m{var:compression \u001b[39mfor\u001b[39;00m var \u001b[39min\u001b[39;00m pa_O3\u001b[39m.\u001b[39mdata_vars}\n\u001b[0;32m      7\u001b[0m pa_O3\u001b[39m.\u001b[39mto_netcdf(\u001b[39m'\u001b[39m\u001b[39mD:/Download/May_PA_O3.nc\u001b[39m\u001b[39m'\u001b[39m,encoding\u001b[39m=\u001b[39mencoding)\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\xarray\\core\\dataset.py:1903\u001b[0m, in \u001b[0;36mDataset.to_netcdf\u001b[1;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[0;32m   1900\u001b[0m     encoding \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1901\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackends\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m to_netcdf\n\u001b[1;32m-> 1903\u001b[0m \u001b[39mreturn\u001b[39;00m to_netcdf(  \u001b[39m# type: ignore  # mypy cannot resolve the overloads:(\u001b[39;49;00m\n\u001b[0;32m   1904\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1905\u001b[0m     path,\n\u001b[0;32m   1906\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   1907\u001b[0m     \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m,\n\u001b[0;32m   1908\u001b[0m     group\u001b[39m=\u001b[39;49mgroup,\n\u001b[0;32m   1909\u001b[0m     engine\u001b[39m=\u001b[39;49mengine,\n\u001b[0;32m   1910\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   1911\u001b[0m     unlimited_dims\u001b[39m=\u001b[39;49munlimited_dims,\n\u001b[0;32m   1912\u001b[0m     compute\u001b[39m=\u001b[39;49mcompute,\n\u001b[0;32m   1913\u001b[0m     multifile\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1914\u001b[0m     invalid_netcdf\u001b[39m=\u001b[39;49minvalid_netcdf,\n\u001b[0;32m   1915\u001b[0m )\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\xarray\\backends\\api.py:1239\u001b[0m, in \u001b[0;36mto_netcdf\u001b[1;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[39mif\u001b[39;00m multifile:\n\u001b[0;32m   1237\u001b[0m     \u001b[39mreturn\u001b[39;00m writer, store\n\u001b[1;32m-> 1239\u001b[0m writes \u001b[39m=\u001b[39m writer\u001b[39m.\u001b[39;49msync(compute\u001b[39m=\u001b[39;49mcompute)\n\u001b[0;32m   1241\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(target, BytesIO):\n\u001b[0;32m   1242\u001b[0m     store\u001b[39m.\u001b[39msync()\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\xarray\\backends\\common.py:171\u001b[0m, in \u001b[0;36mArrayWriter.sync\u001b[1;34m(self, compute)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marray\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mda\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39m# TODO: consider wrapping targets with dask.delayed, if this makes\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39m# for any discernible difference in perforance, e.g.,\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39m# targets = [dask.delayed(t) for t in self.targets]\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m delayed_store \u001b[39m=\u001b[39m da\u001b[39m.\u001b[39;49mstore(\n\u001b[0;32m    172\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msources,\n\u001b[0;32m    173\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtargets,\n\u001b[0;32m    174\u001b[0m     lock\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlock,\n\u001b[0;32m    175\u001b[0m     compute\u001b[39m=\u001b[39;49mcompute,\n\u001b[0;32m    176\u001b[0m     flush\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    177\u001b[0m     regions\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mregions,\n\u001b[0;32m    178\u001b[0m )\n\u001b[0;32m    179\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msources \u001b[39m=\u001b[39m []\n\u001b[0;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\dask\\array\\core.py:1235\u001b[0m, in \u001b[0;36mstore\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1233\u001b[0m \u001b[39melif\u001b[39;00m compute:\n\u001b[0;32m   1234\u001b[0m     store_dsk \u001b[39m=\u001b[39m HighLevelGraph(layers, dependencies)\n\u001b[1;32m-> 1235\u001b[0m     compute_as_if_collection(Array, store_dsk, map_keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1238\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\dask\\base.py:341\u001b[0m, in \u001b[0;36mcompute_as_if_collection\u001b[1;34m(cls, dsk, keys, scheduler, get, **kwargs)\u001b[0m\n\u001b[0;32m    339\u001b[0m schedule \u001b[39m=\u001b[39m get_scheduler(scheduler\u001b[39m=\u001b[39mscheduler, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m, get\u001b[39m=\u001b[39mget)\n\u001b[0;32m    340\u001b[0m dsk2 \u001b[39m=\u001b[39m optimization_function(\u001b[39mcls\u001b[39m)(dsk, keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 341\u001b[0m \u001b[39mreturn\u001b[39;00m schedule(dsk2, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\dask\\threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[1;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[0;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[1;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[0;32m     90\u001b[0m     pool\u001b[39m.\u001b[39;49msubmit,\n\u001b[0;32m     91\u001b[0m     pool\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[0;32m     92\u001b[0m     dsk,\n\u001b[0;32m     93\u001b[0m     keys,\n\u001b[0;32m     94\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[0;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39;49m_thread_get_id,\n\u001b[0;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39;49mpack_exception,\n\u001b[0;32m     97\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\dask\\local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[1;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    509\u001b[0m         _execute_task(task, data)  \u001b[39m# Re-execute locally\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         raise_exception(exc, tb)\n\u001b[0;32m    512\u001b[0m res, worker_id \u001b[39m=\u001b[39m loads(res_info)\n\u001b[0;32m    513\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m][key] \u001b[39m=\u001b[39m res\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\dask\\local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(exc, tb)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39mif\u001b[39;00m exc\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[0;32m    318\u001b[0m     \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 319\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\dask\\local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[1;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     task, data \u001b[39m=\u001b[39m loads(task_info)\n\u001b[1;32m--> 224\u001b[0m     result \u001b[39m=\u001b[39m _execute_task(task, data)\n\u001b[0;32m    225\u001b[0m     \u001b[39mid\u001b[39m \u001b[39m=\u001b[39m get_id()\n\u001b[0;32m    226\u001b[0m     result \u001b[39m=\u001b[39m dumps((result, \u001b[39mid\u001b[39m))\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\dask\\core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[1;34m(arg, cache, dsk)\u001b[0m\n\u001b[0;32m    115\u001b[0m     func, args \u001b[39m=\u001b[39m arg[\u001b[39m0\u001b[39m], arg[\u001b[39m1\u001b[39m:]\n\u001b[0;32m    116\u001b[0m     \u001b[39m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[39m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[39m# operations in-place.\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m(_execute_task(a, cache) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m args))\n\u001b[0;32m    120\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ishashable(arg):\n\u001b[0;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m arg\n",
      "File \u001b[1;32md:\\APP\\Anaconda3\\lib\\site-packages\\dask\\array\\chunk.py:418\u001b[0m, in \u001b[0;36mgetitem\u001b[1;34m(obj, index)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Getitem function\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \n\u001b[0;32m    399\u001b[0m \u001b[39mThis function creates a copy of the desired selection for array-like\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    415\u001b[0m \n\u001b[0;32m    416\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 418\u001b[0m     result \u001b[39m=\u001b[39m obj[index]\n\u001b[0;32m    419\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    420\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    421\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArray chunk size or shape is unknown. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    422\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPossible solution with x.compute_chunk_sizes()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.96 GiB for an array with shape (744, 38, 138, 135) and data type float32"
     ]
    }
   ],
   "source": [
    "compression=dict(zlib=True,complevel=5)\n",
    "\n",
    "encoding={var:compression for var in chem.data_vars}\n",
    "chem.to_netcdf('D:/Download/May_chem.nc',encoding=encoding)\n",
    "\n",
    "encoding={var:compression for var in pa_O3.data_vars}\n",
    "pa_O3.to_netcdf('D:/Download/May_PA_O3.nc',encoding=encoding)\n",
    "\n",
    "encoding={var:compression for var in isam_O3.data_vars}\n",
    "isam_O3.to_netcdf('D:/Download/May_ISAM_O3.nc',encoding=encoding)\n",
    "\n",
    "encoding={var:compression for var in pa_NOx.data_vars}\n",
    "pa_NOx.to_netcdf('D:/Download/May_PA_NOx.nc',encoding=encoding)\n",
    "\n",
    "encoding={var:compression for var in isam_PM.data_vars}\n",
    "isam_PM.to_netcdf('D:/Download/May_ISAM_PM.nc',encoding=encoding)\n",
    "\n",
    "encoding={var:compression for var in met.data_vars}\n",
    "met.to_netcdf('D:/Download/May_met.nc',encoding=encoding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
